{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a7be427a-6628-4150-a355-9dc45edd144d",
   "metadata": {},
   "source": [
    "What is NLP?\n",
    "Natural Language Processing (NLP) is a field of AI that helps the computers understand and process human languages.\n",
    "\n",
    "\n",
    "What is NLTK?\n",
    "Natural Language ToolKit is one of the most popular Python Libraries for:\n",
    "1. Tokenization\n",
    "2. Stemming and Lemmatization\n",
    "3. POS Tagging\n",
    "4. NER\n",
    "5. Parsing\n",
    "6. Corpora access\n",
    "\n",
    "Why use NLTK?\n",
    "1. Beginner Friendly\n",
    "2. Large Built in Datasets\n",
    "3. Pretrained POS taggers\n",
    "4. Quick Experimentation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d80b7c8-f08b-4227-9301-5847f7d907ce",
   "metadata": {},
   "source": [
    "Text Preprocessing:\n",
    "\n",
    "The method of preparing the text before using it for NLP/\n",
    "\n",
    "Common steps:\n",
    "1. Lowercasing\n",
    "2. Removing punctuation (., !, ;,)\n",
    "3. Tokenization\n",
    "4. Stopword Removal\n",
    "5. Stemming / Lemmatization\n",
    "6. Removing Numbers / Special Chars\n",
    "7. Vectorization (Bag of Words, TF-IDF, Word Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d2d038-6a8f-40f2-a956-031f09c12927",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 245 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/ssharma/Desktop/python_tutorial/data_science/.venv/lib/python3.9/site-packages (from nltk) (1.5.2)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2025.11.3-cp39-cp39-macosx_11_0_arm64.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/ssharma/Desktop/python_tutorial/data_science/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16fc736a-1928-4f38-afd5-749085bedd71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK implementation\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0632fbb5-f7fc-4542-838b-41e8e3e9d2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4651664a-709d-45fa-b5ac-ca9b30d98c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLTK is an amazing library for learning deep learning and NLP. It makes performing NLP and text preprocessing much easiers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823e28d6-f2e1-47fe-afa5-406f78305d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nltk is an amazing library for learning deep learning and nlp. it makes performing nlp and text preprocessing much easiers'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercasing\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77705890-d8bc-40f8-af04-bbcb7010e0da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'library',\n",
       " 'for',\n",
       " 'learning',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'nlp',\n",
       " '.',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'performing',\n",
       " 'nlp',\n",
       " 'and',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " 'much',\n",
       " 'easiers']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2f2e87-d925-40be-8471-8e8f8fe52efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a998ae3-d94d-4f4c-997b-7b739562bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuations\n",
    "final_tokens = []\n",
    "for t in tokens:\n",
    "    if t not in string.punctuation:\n",
    "        final_tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287e4b4f-dfed-411a-b607-19c029e02668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'is',\n",
       " 'an',\n",
       " 'amazing',\n",
       " 'library',\n",
       " 'for',\n",
       " 'learning',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'and',\n",
       " 'nlp',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'performing',\n",
       " 'nlp',\n",
       " 'and',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " 'much',\n",
       " 'easiers']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84663a49-4761-4042-ad24-6c05116956ba",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "670e1316-49e0-467b-bf0d-3ab3d1326765",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'amazing',\n",
       " 'library',\n",
       " 'learning',\n",
       " 'deep',\n",
       " 'learning',\n",
       " 'nlp',\n",
       " 'makes',\n",
       " 'performing',\n",
       " 'nlp',\n",
       " 'text',\n",
       " 'preprocessing',\n",
       " 'much',\n",
       " 'easiers']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_removed = []\n",
    "\n",
    "for t in final_tokens:\n",
    "    if t not in stop_words:\n",
    "        stop_words_removed.append(t)\n",
    "\n",
    "stop_words_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2386066e-b29d-43c0-95c7-8f7a4634225b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'amaz',\n",
       " 'librari',\n",
       " 'learn',\n",
       " 'deep',\n",
       " 'learn',\n",
       " 'nlp',\n",
       " 'make',\n",
       " 'perform',\n",
       " 'nlp',\n",
       " 'text',\n",
       " 'preprocess',\n",
       " 'much',\n",
       " 'easier']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stems = []\n",
    "\n",
    "for t in stop_words_removed:\n",
    "    stems.append(stemmer.stem(t))\n",
    "\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69335126-7f2c-4a86-a1ed-cc2218ab43e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'amaz',\n",
       " 'librari',\n",
       " 'learn',\n",
       " 'deep',\n",
       " 'learn',\n",
       " 'nlp',\n",
       " 'make',\n",
       " 'perform',\n",
       " 'nlp',\n",
       " 'text',\n",
       " 'preprocess',\n",
       " 'much',\n",
       " 'easier']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemm = WordNetLemmatizer()\n",
    "\n",
    "lemms = []\n",
    "for t in stems:\n",
    "    lemms.append(lemm.lemmatize(t))\n",
    "\n",
    "lemms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a02d9a47-7bdd-4165-b3f1-8d4b4087d34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging (Parts of Speech Tagging)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e63ff8a8-b197-4dd0-9826-bc4b4d98febd",
   "metadata": {},
   "source": [
    "POS Tagging assigns grammatical labels to each word:\n",
    "1. NN -> Noun\n",
    "2. VB -> Verb\n",
    "3. JJ -> Adjective\n",
    "4. RB -> Adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f2443c-5f24-4e09-add1-a933e872416d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('quick', 'JJ'),\n",
       " ('brown', 'NN'),\n",
       " ('fox', 'NN'),\n",
       " ('jumps', 'VBZ'),\n",
       " ('over', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('lazy', 'JJ'),\n",
       " ('dog', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the very lazy dog\"\n",
    "sentence = word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(sentence)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ce7be12-a3e9-4629-8296-5812b1c185f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brown\n",
      "fox\n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "# Extract the nouns only\n",
    "for tupes in pos_tags:\n",
    "    if tupes[1] == 'NN':\n",
    "        print(tupes[0])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd47675f-6d99-4412-aa14-711c69ff971c",
   "metadata": {},
   "source": [
    "Named Entity Recognition:\n",
    "\n",
    "NER identifies entities such as:\n",
    "1. PERSON\n",
    "2. ORGANIZATION\n",
    "3. LOCATION\n",
    "4. DATE\n",
    "5. MONEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2160924-9316-4378-a355-8d9c07cebd7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting svgling\n",
      "  Downloading svgling-0.5.0-py3-none-any.whl (31 kB)\n",
      "Collecting svgwrite\n",
      "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "\u001b[K     |████████████████████████████████| 67 kB 847 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
      "Successfully installed svgling-0.5.0 svgwrite-1.4.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/ssharma/Desktop/python_tutorial/data_science/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4eda239-8191-4dbf-8329-d0880ce500ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data] Downloading package words to /Users/ssharma/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3590c4e-d9e5-4829-9238-b0472c9a23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Barrack obama was born in Hawaii and served as the president of the USA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd5ef195-7e16-4235-932e-8e5e3dd0ede1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,688.0,120.0\" width=\"688px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"10.4651%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">barrack</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.23256%\" y1=\"20px\" y2=\"48px\" /><svg width=\"8.13953%\" x=\"10.4651%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">obama</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"14.5349%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.81395%\" x=\"18.6047%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.5116%\" y1=\"20px\" y2=\"48px\" /><svg width=\"6.97674%\" x=\"24.4186%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">born</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.907%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.65116%\" x=\"31.3953%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.7209%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.30233%\" x=\"36.0465%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">hawaii</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.6977%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.81395%\" x=\"45.3488%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"48.2558%\" y1=\"20px\" y2=\"48px\" /><svg width=\"9.30233%\" x=\"51.1628%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">served</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.814%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.65116%\" x=\"60.4651%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">as</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.7907%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.81395%\" x=\"65.1163%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.0233%\" y1=\"20px\" y2=\"48px\" /><svg width=\"12.7907%\" x=\"70.9302%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">president</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"77.3256%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.65116%\" x=\"83.7209%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"86.0465%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.81395%\" x=\"88.3721%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.2791%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.81395%\" x=\"94.186%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">usa</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.093%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('barrack', 'NN'), ('obama', 'NN'), ('was', 'VBD'), ('born', 'VBN'), ('in', 'IN'), ('hawaii', 'NN'), ('and', 'CC'), ('served', 'VBD'), ('as', 'IN'), ('the', 'DT'), ('president', 'NN'), ('of', 'IN'), ('the', 'DT'), ('usa', 'NN')])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = sentence.lower()\n",
    "tokens = word_tokenize(sentence)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "entities = nltk.ne_chunk(tags)\n",
    "entities"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fda085ad-f4ac-4871-8b37-3761ef1eadd1",
   "metadata": {},
   "source": [
    "What is LSA?\n",
    "\n",
    "Latent Semantic Analysis is a technique in NLP and Information Retrieval Systems that uncovers the hidden (latent) relationships between words and documents.\n",
    "\n",
    "It is based on the idea that Words with similar meanings appear in similar contexts.\n",
    "\n",
    "LSA uses linear algebra (Singular Value Decomposition) to reduce the dimensionality of text data and represent documents in a semantic space.\n",
    "\n",
    "\n",
    "How LSA Works?\n",
    "1. Step 1 -> Create term document matrix\n",
    "Using TF-IDF or Bag Of Words\n",
    "a. Rows -> Words\n",
    "b. Columns -> Documents\n",
    "c. Values -> frequency of TF-IDF score\n",
    "\n",
    "2. Step 2 -> Apply SVD (Singular Value Decomposition)\n",
    "SVD breaks the matrix into 3 smaller matrices:\n",
    "    A = UΣV^T\n",
    "    where, \n",
    "    U -> word-topic matrix\n",
    "    V^T -> topic-document matrix\n",
    "    Σ -> diagonal matrix with singular values\n",
    "\n",
    "3. Step 3 -> Keep Top K Dimensions (Latent Topics)\n",
    "Reduce Σ by keeping only top k singular values.\n",
    "This produces a lower-dimensional semantic space, capturing the important concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1cde933-7760-46d2-8c1a-bd77cf34a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"Dogs and cats are great pets\",\n",
    "    \"The dog chased the cat\",\n",
    "    \"The mat was sat on by the cat\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bafb157b-2b01-4cce-832a-29c060c3fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3880cde2-3b60-45c5-a668-ad02a803c7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english') \n",
    "X = vectorizer.fit_transform(documents)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "242f6f9b-a7f5-4044-911e-54aa7e999dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dc266ee-e86c-4490-95af-b89a069900c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.84398672e-01,  7.99397792e-17],\n",
       "       [ 6.53711544e-17,  1.00000000e+00],\n",
       "       [ 3.73428975e-01, -2.63262306e-16],\n",
       "       [ 9.84398672e-01,  7.99397792e-17]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSA using SVD\n",
    "lsa = TruncatedSVD(n_components=2, random_state=51)\n",
    "X_lsa = lsa.fit_transform(X)\n",
    "X_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "647e4f51-e708-4dc7-98cd-ec9cc5671abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>9.843987e-01</td>\n",
       "      <td>7.993978e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>6.537115e-17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>3.734290e-01</td>\n",
       "      <td>-2.632623e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>9.843987e-01</td>\n",
       "      <td>7.993978e-17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Topic1        Topic2\n",
       "Doc1  9.843987e-01  7.993978e-17\n",
       "Doc2  6.537115e-17  1.000000e+00\n",
       "Doc3  3.734290e-01 -2.632623e-16\n",
       "Doc4  9.843987e-01  7.993978e-17"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe to visualize the 2D semantic representation\n",
    "df = pd.DataFrame(X_lsa, columns=['Topic1', 'Topic2'])\n",
    "df.index = [f\"Doc{i+1}\" for i in range(len(documents))]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25c4408a-bebf-4a21-912c-54d141da0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Similarity using cosine similarity\n",
    "similarity = cosine_similarity(X_lsa)\n",
    "sim_df = pd.DataFrame(similarity, index=df.index, columns=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0754df5e-b7d9-4562-97ca-dfb251e9e580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc1</th>\n",
       "      <th>Doc2</th>\n",
       "      <th>Doc3</th>\n",
       "      <th>Doc4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Doc1</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.465779e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc2</th>\n",
       "      <td>1.465779e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-6.396151e-16</td>\n",
       "      <td>1.465779e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc3</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-6.396151e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doc4</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.465779e-16</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Doc1          Doc2          Doc3          Doc4\n",
       "Doc1  1.000000e+00  1.465779e-16  1.000000e+00  1.000000e+00\n",
       "Doc2  1.465779e-16  1.000000e+00 -6.396151e-16  1.465779e-16\n",
       "Doc3  1.000000e+00 -6.396151e-16  1.000000e+00  1.000000e+00\n",
       "Doc4  1.000000e+00  1.465779e-16  1.000000e+00  1.000000e+00"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "31841e77-a44c-40cd-a66d-9294873c3935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for each LSA Topic:\n",
      "Topic 1 Top Words: \n",
      "['sat', 'mat', 'cat', 'chased', 'dog']\n",
      "Topic 2 Top Words: \n",
      "['pets', 'great', 'dogs', 'cats', 'sat']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Show top contributing terms to each topic\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "components = lsa.components_\n",
    "\n",
    "print(\"Top Terms for each LSA Topic:\")\n",
    "for i, comp in enumerate(components):\n",
    "    term_indices = comp.argsort()[::-1][:5]\n",
    "    print(f\"Topic {i+1} Top Words: \")\n",
    "    print([terms[idx] for idx in term_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Data Science Environment Morning",
   "language": "python",
   "name": "ds_morning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
